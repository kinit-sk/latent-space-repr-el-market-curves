# Latent Space Representation of Electricity Market Curves

<a target="_blank" href="https://cookiecutter-data-science.drivendata.org/">
    <img src="https://img.shields.io/badge/CCDS-Project%20template-328F97?logo=cookiecutter" />
</a>

This repository contains the code for the paper **"Latent Space Representation of Electricity Market Curves: Maintaining Structural Integrity"** by Martin VÃ½boh, Zuzana ChladnÃ¡, Gabriela GrmanovÃ¡, and MÃ¡ria LuckÃ¡.

ðŸ“„ **Paper**: [arXiv preprint](https://arxiv.org/abs/2503.11294v2)

## Getting Started

### Installation

```bash
conda env create -f environment.yml

conda activate curves_env
```

### Running the Pipeline

The dimensionality reduction pipeline consists of four main steps:

1. **Train dimensionality reduction models**
```bash
python -m curves.dim-reduct.train
```
Trains PCA, kPCA, UMAP, or Autoencoder models on supply and demand curves. Model selection and hyperparameters are to be configured in `config.yml`.

2. **Generate reconstructions with moving window retraining**
```bash
python -m curves.dim-reduct.predict
```
Applies trained models to test data with periodic retraining to account for potential temporal context drifts.

3. **Apply isotonic transformation (optional)**
```bash
python -m curves.dim-reduct.isotonic_transform
```
Enforces monotonicity constraints on reconstructed curves using isotonic regression.

4. **Evaluate reconstruction quality**
```bash
python -m curves.dim-reduct.evaluate
```
Calculates RMSE, MAE, Bias, and WAPE metrics overall and by time periods (hourly, weekday).

### Configuration

Edit `config.yml` to specify:
- Dataset paths and date ranges
- Dimensionality reduction method (pca/kpca/umap/autoencoder)
- Number of components for supply and demand
- Evaluation settings (retrain interval, monotonic evaluation)

## Project Organization

```
â”œâ”€â”€ LICENSE            <- Open-source license if one is chosen
â”œâ”€â”€ Makefile           <- Makefile with convenience commands like `make data` or `make train`
â”œâ”€â”€ README.md          <- The top-level README for developers using this project.
â”œâ”€â”€ data
â”‚   â”œâ”€â”€ external       <- Data from third party sources.
â”‚   â”œâ”€â”€ interim        <- Intermediate data that has been transformed.
â”‚   â”œâ”€â”€ processed      <- The final, canonical data sets for modeling.
â”‚   â””â”€â”€ raw            <- The original, immutable data dump.
â”‚
â”œâ”€â”€ docs               <- A default mkdocs project; see www.mkdocs.org for details
â”‚
â”œâ”€â”€ models             <- Trained and serialized models, model predictions, or model summaries
â”‚
â”œâ”€â”€ notebooks          <- Jupyter notebooks. Naming convention is a number (for ordering),
â”‚                         the creator's initials, and a short `-` delimited description, e.g.
â”‚                         `1.0-jqp-initial-data-exploration`.
â”‚
â”œâ”€â”€ pyproject.toml     <- Project configuration file with package metadata for 
â”‚                         curves and configuration for tools like black
â”‚
â”œâ”€â”€ references         <- Data dictionaries, manuals, and all other explanatory materials.
â”‚
â”œâ”€â”€ reports            <- Generated analysis as HTML, PDF, LaTeX, etc.
â”‚   â””â”€â”€ figures        <- Generated graphics and figures to be used in reporting
â”‚
â”œâ”€â”€ environment.yml   <- The requirements file for reproducing the environment
â”‚
â”œâ”€â”€ setup.cfg          <- Configuration file for flake8
â”‚
â””â”€â”€ curves   <- Source code for use in this project.
    â”‚
    â”œâ”€â”€ __init__.py
    â”‚
    â”œâ”€â”€ autoencoder.py          <- Code with AutoEncoder model class definitions.
    â”‚
    â”œâ”€â”€ config.py               <- Store useful variables and configuration
    â”‚
    â”œâ”€â”€ dataset.py              <- Scripts to download or generate data
    â”‚
    â”œâ”€â”€ features.py             <- Code to create features for modeling
    â”‚
    â”œâ”€â”€ dim-reduct              <- Dimensionality reduction pipeline
    â”‚   â”œâ”€â”€ __init__.py
    â”‚   â”œâ”€â”€ train.py            <- Train dimensionality reduction models (PCA, kPCA, UMAP, Autoencoder)
    â”‚   â”œâ”€â”€ predict.py          <- Generate reconstructions using trained models with moving window retraining
    â”‚   â”œâ”€â”€ evaluate.py         <- Calculate reconstruction metrics (RMSE, MAE, Bias, WAPE) overall and by time periods
    â”‚   â””â”€â”€ isotonic_transform.py <- Apply isotonic regression to enforce monotonicity on reconstructed curves
    â”‚
    â””â”€â”€ plots.py                <- Code to create visualizations
```

--------

